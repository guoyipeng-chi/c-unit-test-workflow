# ğŸŒ vLLM è¿œç¨‹éƒ¨ç½²é…ç½®æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1ï¸âƒ£: ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰ â­

**Linux/macOS:**
```bash
# ä¸´æ—¶è®¾ç½®ï¼ˆå½“å‰ç»ˆç«¯æœ‰æ•ˆï¼‰
export VLLM_API_BASE=http://192.168.1.100:8000
export VLLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

# æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶
source vllm_config.env

# éªŒè¯é…ç½®
python quickstart_llm.py --check
```

**Windows PowerShell:**
```powershell
# ä¸´æ—¶è®¾ç½®
$env:VLLM_API_BASE = "http://192.168.1.100:8000"
$env:VLLM_MODEL = "Qwen/Qwen2.5-Coder-32B-Instruct"

# æˆ–ä½¿ç”¨é…ç½®è„šæœ¬
.\vllm_config.ps1

# éªŒè¯é…ç½®
python quickstart_llm.py --check
```

**Windows CMD:**
```cmd
REM ä¸´æ—¶è®¾ç½®
set VLLM_API_BASE=http://192.168.1.100:8000
set VLLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

REM æˆ–ä½¿ç”¨æ‰¹å¤„ç†è„šæœ¬
vllm_config.bat

REM éªŒè¯é…ç½®
python quickstart_llm.py --check
```

---

### æ–¹å¼ 2ï¸âƒ£: å‘½ä»¤è¡Œå‚æ•°

```bash
# ç›´æ¥åœ¨å‘½ä»¤è¡ŒæŒ‡å®š
python tools/ut_workflow_llm.py \
  --llm-api http://192.168.1.100:8000 \
  --llm-model Qwen/Qwen2.5-Coder-32B-Instruct \
  --functions validate_student_name
```

---

### æ–¹å¼ 3ï¸âƒ£: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `llm_workflow_config.json`:
```json
{
  "llm": {
    "api_base": "http://192.168.1.100:8000",
    "model": "Qwen/Qwen2.5-Coder-32B-Instruct"
  }
}
```

---

## é…ç½®ä¼˜å…ˆçº§

```
ç¯å¢ƒå˜é‡ > å‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
```

**ç¤ºä¾‹ï¼š**
```bash
# å‡è®¾é…ç½®æ–‡ä»¶ä¸­: "api_base": "http://localhost:8000"
# è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_API_BASE=http://192.168.1.100:8000

# è¿è¡Œï¼ˆå°†ä½¿ç”¨ç¯å¢ƒå˜é‡çš„åœ°å€ï¼‰
python quickstart_llm.py --generate
# â†’ å®é™…è¿æ¥: http://192.168.1.100:8000 âœ“
```

---

## å¸¸è§éƒ¨ç½²åœºæ™¯

### åœºæ™¯ 1: æœ¬åœ°å¼€å‘ï¼ˆé»˜è®¤ï¼‰
```bash
# ä¸è®¾ç½®ä»»ä½•é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
python quickstart_llm.py --generate
# â†’ è¿æ¥: http://localhost:8000
```

---

### åœºæ™¯ 2: å†…ç½‘æœåŠ¡å™¨
```bash
# æœåŠ¡å™¨IP: 192.168.1.100
export VLLM_API_BASE=http://192.168.1.100:8000
python quickstart_llm.py --generate
```

**vLLMæœåŠ¡å™¨ç«¯å¯åŠ¨å‘½ä»¤ï¼š**
```bash
# åœ¨æœåŠ¡å™¨ä¸Šå¯åŠ¨vLLM
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-Coder-32B-Instruct \
  --host 0.0.0.0 \
  --port 8000
```

---

### åœºæ™¯ 3: äº‘æœåŠ¡å™¨ï¼ˆå…¬ç½‘ï¼‰
```bash
# ä½¿ç”¨åŸŸå
export VLLM_API_BASE=http://vllm.yourcompany.com:8000
python quickstart_llm.py --generate
```

æˆ–ä½¿ç”¨IP:
```bash
export VLLM_API_BASE=http://123.45.67.89:8000
python quickstart_llm.py --generate
```

---

### åœºæ™¯ 4: SSH ç«¯å£è½¬å‘
å¦‚æœæœåŠ¡å™¨é˜²ç«å¢™é™åˆ¶ï¼Œä½¿ç”¨SSHéš§é“ï¼š

```bash
# æœ¬åœ°ç»ˆç«¯1: å»ºç«‹éš§é“
ssh -L 8000:localhost:8000 user@remote-server

# æœ¬åœ°ç»ˆç«¯2: ä½¿ç”¨æœ¬åœ°åœ°å€
export VLLM_API_BASE=http://localhost:8000
python quickstart_llm.py --generate
# â†’ å®é™…é€šè¿‡SSHè½¬å‘åˆ°è¿œç¨‹æœåŠ¡å™¨
```

---

### åœºæ™¯ 5: Dockerå®¹å™¨
```bash
# å¯åŠ¨vLLMå®¹å™¨ï¼ˆæœåŠ¡å™¨ç«¯ï¼‰
docker run -d --gpus all \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen2.5-Coder-32B-Instruct

# å®¢æˆ·ç«¯è¿æ¥
export VLLM_API_BASE=http://server-ip:8000
python quickstart_llm.py --generate
```

---

## éªŒè¯è¿æ¥

### æ–¹æ³• 1: ä½¿ç”¨å·¥å…·è‡ªå¸¦æ£€æŸ¥
```bash
python quickstart_llm.py --check
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
âœ“ Python version: 3.12.0
âœ“ vLLM service: Connected to http://192.168.1.100:8000
âœ“ compile_commands.json: Found
...
```

---

### æ–¹æ³• 2: æ‰‹åŠ¨æµ‹è¯•API
```bash
# æµ‹è¯• /v1/models ç«¯ç‚¹
curl http://192.168.1.100:8000/v1/models

# é¢„æœŸè¾“å‡º:
# {"object":"list","data":[{"id":"Qwen/Qwen2.5-Coder-32B-Instruct",...}]}
```

---

### æ–¹æ³• 3: Pythonè„šæœ¬æµ‹è¯•
```python
import os
from tools.llm_client import VLLMClient

# æµ‹è¯•è¿æ¥
client = VLLMClient()  # è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡
print(f"è¿æ¥åˆ°: {client.api_base}")
print(f"æ¨¡å‹: {client.model}")
```

---

## ç¯å¢ƒå˜é‡åˆ—è¡¨

| å˜é‡å | è¯´æ˜ | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|--------|------|--------|------|
| `VLLM_API_BASE` | vLLMæœåŠ¡åœ°å€ | `http://localhost:8000` | `http://192.168.1.100:8000` |
| `VLLM_MODEL` | æ¨¡å‹åç§° | `qwen-coder` | `Qwen/Qwen2.5-Coder-32B-Instruct` |
| `VLLM_API_KEY` | APIå¯†é’¥ï¼ˆå¦‚éœ€è¦ï¼‰ | `dummy` | `sk-xxxxx` |
| `VLLM_TIMEOUT` | è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ | `120` | `180` |

---

## å¸¸è§é—®é¢˜

### Q1: è¿æ¥è¢«æ‹’ç» (Connection refused)
**æ’æŸ¥æ­¥éª¤ï¼š**
```bash
# 1. æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
curl http://server-ip:8000/v1/models

# 2. æ£€æŸ¥é˜²ç«å¢™
# æœåŠ¡å™¨ç«¯ï¼š
sudo ufw allow 8000/tcp

# 3. æ£€æŸ¥vLLMæ˜¯å¦ç»‘å®š0.0.0.0
# vLLMå¯åŠ¨æ—¶å¿…é¡»ï¼š--host 0.0.0.0
```

---

### Q2: è¶…æ—¶ (Timeout)
**è§£å†³æ–¹æ³•ï¼š**
```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export VLLM_TIMEOUT=300

# æˆ–å‘½ä»¤è¡Œ
python tools/ut_workflow_llm.py --llm-timeout 300
```

---

### Q3: æ¨¡å‹æ‰¾ä¸åˆ° (Model not found)
**æ£€æŸ¥ï¼š**
```bash
# 1. æŸ¥çœ‹æœåŠ¡å™¨æ”¯æŒçš„æ¨¡å‹
curl http://server-ip:8000/v1/models | jq

# 2. ç¡®ä¿æ¨¡å‹åç§°å®Œå…¨åŒ¹é…
export VLLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct  # ç²¾ç¡®åç§°
```

---

### Q4: ç¯å¢ƒå˜é‡ä¸ç”Ÿæ•ˆ
**æ£€æŸ¥ï¼š**
```bash
# Linux/macOS
echo $VLLM_API_BASE

# Windows PowerShell
echo $env:VLLM_API_BASE

# Windows CMD
echo %VLLM_API_BASE%

# å¦‚æœä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰æ­£ç¡®è®¾ç½®
```

---

## æ€§èƒ½ä¼˜åŒ–

### ç½‘ç»œå»¶è¿Ÿä¼˜åŒ–
```bash
# 1. ä½¿ç”¨å†…ç½‘åœ°å€ï¼ˆæ¯”å…¬ç½‘å¿«ï¼‰
export VLLM_API_BASE=http://192.168.1.100:8000  # âœ“ å†…ç½‘
# è€Œä¸æ˜¯:
# export VLLM_API_BASE=http://123.45.67.89:8000  # âœ— å…¬ç½‘

# 2. å‡å°‘max_tokens
# ç¼–è¾‘ llm_workflow_config.json:
"max_tokens": 2048  # ä»4096å‡å°‘åˆ°2048
```

---

### æ‰¹é‡ç”Ÿæˆä¼˜åŒ–
```bash
# ç”Ÿæˆå¤šä¸ªå‡½æ•°æ—¶ï¼Œåœ¨æœåŠ¡å™¨é™„è¿‘è¿è¡Œ
# å¦‚æœå»¶è¿Ÿé«˜ï¼Œè€ƒè™‘ï¼š
# 1. åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šcloneé¡¹ç›®å¹¶è¿è¡Œ
# 2. ä½¿ç”¨SSHç«¯å£è½¬å‘å‡å°‘å»¶è¿Ÿ
```

---

## å®‰å…¨å»ºè®®

### 1. ä½¿ç”¨API Key
```bash
# æœåŠ¡å™¨ç«¯å¯åŠ¨vLLMæ—¶è®¾ç½®å¯†é’¥
python -m vllm.entrypoints.openai.api_server \
  --api-key your-secret-key \
  ...

# å®¢æˆ·ç«¯é…ç½®
export VLLM_API_KEY=your-secret-key
```

---

### 2. ç½‘ç»œéš”ç¦»
- âœ… å†…ç½‘éƒ¨ç½²æœ€å®‰å…¨
- âš ï¸ å…¬ç½‘éƒ¨ç½²å¿…é¡»åŠ API Key
- âš ï¸ ä½¿ç”¨HTTPSä»£æ›¿HTTPï¼ˆnginxåå‘ä»£ç†ï¼‰

---

### 3. é˜²ç«å¢™é…ç½®
```bash
# æœåŠ¡å™¨ç«¯ï¼šä»…å…è®¸ç‰¹å®šIP
sudo ufw allow from 192.168.1.0/24 to any port 8000
```

---

## å®Œæ•´ç¤ºä¾‹

### ä»é›¶å¼€å§‹é…ç½®è¿œç¨‹vLLM

**æœåŠ¡å™¨ç«¯ï¼ˆ192.168.1.100ï¼‰ï¼š**
```bash
# 1. å®‰è£…vLLM
pip install vllm

# 2. å¯åŠ¨æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-Coder-32B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 1

# 3. éªŒè¯
curl http://localhost:8000/v1/models
```

**å®¢æˆ·ç«¯ï¼ˆä½ çš„å¼€å‘æœºï¼‰ï¼š**
```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
export VLLM_API_BASE=http://192.168.1.100:8000
export VLLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

# 2. éªŒè¯è¿æ¥
python quickstart_llm.py --check

# 3. ç”Ÿæˆæµ‹è¯•
python quickstart_llm.py --generate
```

---

## é…ç½®æ–‡ä»¶æ¨¡æ¿

é¡¹ç›®æä¾›äº†3ä¸ªé…ç½®æ–‡ä»¶æ¨¡æ¿ï¼š

| æ–‡ä»¶ | å¹³å° | ä½¿ç”¨æ–¹æ³• |
|------|------|----------|
| `vllm_config.env` | Linux/macOS | `source vllm_config.env` |
| `vllm_config.ps1` | Windows PowerShell | `.\vllm_config.ps1` |
| `vllm_config.bat` | Windows CMD | `vllm_config.bat` |

æ ¹æ®éœ€è¦ç¼–è¾‘è¿™äº›æ–‡ä»¶ï¼Œç„¶åè¿è¡Œå³å¯ï¼

---

**æ›´å¤šå¸®åŠ©ï¼š**
- æŸ¥çœ‹ `LLM_WORKFLOW_GUIDE.md` - vLLMéƒ¨ç½²è¯¦è§£
- æŸ¥çœ‹ `QUICKREF_LLM.md` - å¿«é€Ÿå‚è€ƒ
- è¿è¡Œ `python quickstart_llm.py --help` - å‘½ä»¤å¸®åŠ©
